# Daily Interview Questions - Bert & GPT

### BERT和GPT区别(2024.9.22)

参考：[算法工程师面试八股（搜广推方向）_搜广推八股-CSDN博客](https://blog.csdn.net/keiven_/article/details/134758737)

二者都是采用「预训练+微调」的范式，

BERT：几乎就是为「无监督预训练+下游任务微调」的范式量身定制的模型。BERT使用的掩码语言模型任务没有生成文本的能力，但换来的是双向编码的能力，这让模型拥有了更强的文本编码性能。主要用于解决语言理解相关的任务，如问答、语义关系抽取等。BERT更适用于在已有标注数据上微调的场景。

GPT：基于生成式预训练的思想开发，为了保留生成文本的能力，只能采用单向编码。主要用于解决语言生成相关的任务，如文本生成、机器翻译等。GPT更适用于在大量未标注数据上预训练的场景。