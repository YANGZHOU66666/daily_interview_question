# Interview Questions - Transformer

### 为什么Query和Key的点乘需要除以$\sqrt{d_k}$？（$\sqrt{d_k}$是key向量的维数）(2024.9.7)

在softmax函数中，较大的输入值会导致更加尖锐的概率分布，而较小的值会导致更加平坦的分布。如果没有缩放，当$\sqrt{d_k}$的值很大时，点积的结果也会很大，这可能导致`softmax`函数的梯度非常小（接近于0），从而使得模型难以学习

一句话概括：`softmax`参数值特别大可能会梯度消失，使梯度大小更加稳定



### Transformer为何使用多头注意力机制？(2024.9.14)

参考：[Transformer八股！！！常见八股、台大李宏毅图解、个人理解-CSDN博客](https://blog.csdn.net/weixin_45995838/article/details/139855696)

**并行计算：**多头注意力机制允许模型同时关注输入序列的不同部分，每个注意力头可以独立计算，从而实现更高效的并行计算。这样能够加快模型的训练速度。

**提升表征能力：**通过引入多个注意力头，模型可以学习到不同类型的注意力权重，从而捕捉输入序列中不同层次、不同方面的语义信息。这有助于提升模型对输入序列的表征能力。

**降低过拟合风险：**多头注意力机制使得模型可以综合不同角度的信息，从而提高泛化能力，降低过拟合的风险。

**增强模型解释性：**每个注意力头可以关注输入序列的不同部分，因此可以更好地理解模型对于不同输入信息的关注程度，使得模型的决策更具解释性。



### 在Transformer 模型中，为什么要使用 Dropout？(2024.9.15)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

`Dropout`是一种正则化技术，通过随机丢弃一部分网络连接，可以有效减少模型的过拟合，增强模型的泛化能力。



### Transformer模型中自注意力机制的数学表达式是什么？(2024.9.16)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

自注意力机制的数学表达式可以表示为：$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$其中，`Q`、`K`和V分别代表查询（query）、键（key）和值（value）矩阵， dk 是键向量的维度。此表达式通过将查询和键的点积除以 $d_k$ 进行缩放，然后应用 $softmax$ 函数来获取权重，最后这些权重用于值向量的加权求和。
