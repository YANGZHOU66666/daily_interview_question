# Interview Questions - Transformer

### 为什么Query和Key的点乘需要除以$\sqrt{d_k}$？（$\sqrt{d_k}$是key向量的维数）(2024.9.7)

在softmax函数中，较大的输入值会导致更加尖锐的概率分布，而较小的值会导致更加平坦的分布。如果没有缩放，当$\sqrt{d_k}$的值很大时，点积的结果也会很大，这可能导致`softmax`函数的梯度非常小（接近于0），从而使得模型难以学习

一句话概括：`softmax`参数值特别大可能会梯度消失，使梯度大小更加稳定



### Transformer为何使用多头注意力机制？(2024.9.14)

参考：[Transformer八股！！！常见八股、台大李宏毅图解、个人理解-CSDN博客](https://blog.csdn.net/weixin_45995838/article/details/139855696)

**并行计算：**多头注意力机制允许模型同时关注输入序列的不同部分，每个注意力头可以独立计算，从而实现更高效的并行计算。这样能够加快模型的训练速度。

**提升表征能力：**通过引入多个注意力头，模型可以学习到不同类型的注意力权重，从而捕捉输入序列中不同层次、不同方面的语义信息。这有助于提升模型对输入序列的表征能力。

**降低过拟合风险：**多头注意力机制使得模型可以综合不同角度的信息，从而提高泛化能力，降低过拟合的风险。

**增强模型解释性：**每个注意力头可以关注输入序列的不同部分，因此可以更好地理解模型对于不同输入信息的关注程度，使得模型的决策更具解释性。



### 在Transformer 模型中，为什么要使用 Dropout？(2024.9.15)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

`Dropout`是一种正则化技术，通过随机丢弃一部分网络连接，可以有效减少模型的过拟合，增强模型的泛化能力。



### Transformer模型中自注意力机制的数学表达式是什么？(2024.9.16)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

自注意力机制的数学表达式可以表示为：$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$其中，`Q`、`K`和V分别代表查询（query）、键（key）和值（value）矩阵， dk 是键向量的维度。此表达式通过将查询和键的点积除以 $d_k$ 进行缩放，然后应用 $softmax$ 函数来获取权重，最后这些权重用于值向量的加权求和。



### 多头注意力机制如何在数学上被实现，它如何改进自注意力机制的性能？(2024.9.17)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

多头注意力机制首先将`Q`、`K`、`V`通过不同的线性变换映射到不同的表示空间，然后在每个表示空间上独立地应用自注意力机制，最后将所有头的输出拼接并再次线性变换得到最终输出。数学上，设有h个头，对于第i个头，$Head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$最终输出为：$MultiHead(Q,K,V)=Concat(Head_1,...,Head_h)W^O$这种方式能够使模型在不同的子空间捕捉到序列的不同特征，从而提高性能。



### Transformer模型中的位置编码如何数学上表示，它解决了什么问题？(2024.9.18)

参考：[大模型-Transformer 面试八股文，简单背一背 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/689965833)

位置编码通过向每个位置的词嵌入添加一个特定的向量来实现，其中第i个位置的第 `2k` 或 `2k+1`维的位置编码分别为：$PE(i,2k)=sin⁡(i/10000^{2k/d_{model}})、PE(i,2k+1)=cos⁡(i/10000^{2k/d_{model}})$这种正弦和余弦函数的使用使得模型能够区分不同位置的词，并使得模型能够学习到位置的相对关系，解决了`Transformer`模型自身不具备处理序列中元素顺序信息的问题。



### 在Transformer中，如何数学上表达编码器和解码器中的前馈神经网络（FFN）？(2024.9.19)

编码器和解码器中的前馈神经网络（FFN）可以数学上表示为：$FFN(x)=max(0,xW_1+b_1)·W_2+b_2$

其中，$W_1$ 、 $W_2$ 、 $b_1$ 、和 $b_2$ 是网络参数， $max(0,x)$ 代表`ReLU`激活函数。这个`FFN`对每个位置的词向量独立地应用相同的操作，提供了额外的非线性，增加了模型的表达能力。
