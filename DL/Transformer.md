# Interview Questions - Transformer

### 为什么Query和Key的点乘需要除以$\sqrt{d_k}$？（$\sqrt{d_k}$是key向量的维数）(2024.9.7)

在softmax函数中，较大的输入值会导致更加尖锐的概率分布，而较小的值会导致更加平坦的分布。如果没有缩放，当$\sqrt{d_k}$的值很大时，点积的结果也会很大，这可能导致`softmax`函数的梯度非常小（接近于0），从而使得模型难以学习

一句话概括：`softmax`参数值特别大可能会梯度消失，使梯度大小更加稳定



### Transformer为何使用多头注意力机制？(2024.9.14)

参考：[Transformer八股！！！常见八股、台大李宏毅图解、个人理解-CSDN博客](https://blog.csdn.net/weixin_45995838/article/details/139855696)

**并行计算：**多头注意力机制允许模型同时关注输入序列的不同部分，每个注意力头可以独立计算，从而实现更高效的并行计算。这样能够加快模型的训练速度。

**提升表征能力：**通过引入多个注意力头，模型可以学习到不同类型的注意力权重，从而捕捉输入序列中不同层次、不同方面的语义信息。这有助于提升模型对输入序列的表征能力。

**降低过拟合风险：**多头注意力机制使得模型可以综合不同角度的信息，从而提高泛化能力，降低过拟合的风险。

**增强模型解释性：**每个注意力头可以关注输入序列的不同部分，因此可以更好地理解模型对于不同输入信息的关注程度，使得模型的决策更具解释性。

