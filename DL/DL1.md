# Interview Questions-Deep Learning



### 梯度消失和梯度爆炸(2024.9.2)

参考：

[深度学习之3——梯度爆炸与梯度消失 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/68579467)

[深度学习中梯度消失和梯度爆炸的根本原因及其缓解方法_什么是梯度消失-CSDN博客](https://blog.csdn.net/weixin_46470894/article/details/107145207)

[详解机器学习中的梯度消失、爆炸原因及其解决方法_梯度消失的原因及解决方法-CSDN博客](https://blog.csdn.net/qq_25737169/article/details/78847691)

#### 概念

对于深层神经网络$F(x) = f_n(...f_3(f_2(f_1(x)*w+b)*w+b)*w+b)$

$Loss = || g(x) - f(x) ||^2$

$\Delta{w_2}=\frac{\delta{f_n}}{\delta{f_{n-1}}}·\frac{\delta{f_{n-1}}}{\delta{f_{n-2}}}\dots\frac{\delta{f_2}}{\delta{w_2}}$

这里前n-2个项都可看作对激活函数求导，很多激活函数值域在(0,1)，当层数过多时多个导数相乘会趋于0，梯度消失；当激活函数梯度值全在1以上时会梯度爆炸

#### 梯度爆炸的问题

+ 值超出值域(infinity)

对于16位浮点数尤为严重（



+ 对学习率敏感



#### 解决方案

1. 梯度剪切

设置一个梯度剪切阈值；更新梯度时，超过或低于某范围，强制更新到该范围内



2. 权重正则化

$ Loss = (y-W^Tx)^2 +\alpha||W||^2 $



3. 采用其他激活函数

**Relu：**正数部分导数为1，连乘不会趋于0或无穷

**缺点：**

- 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
- 输出不是以0为中心的



**LeakRelu:**
`LeakRelu`就是为了解决Relu的0区间带来的影响，其数学表达为：

$LeakRelu = max(k∗x, x) $

其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。



**elu:**

$elu(x) = x,if(x>0);else:f(x)= \alpha(e^x-1) $



# ##### 4，5，6待施工#####

4. **Batchnorm**









5. **残差网络**





6. **LSTM — 长短期记忆网络**





### L1正则化和L2正则化（2024.9.3）

**参考：**

[L1正则化与L2正则化的区别_l1正则化和l2正则化的区别是-CSDN博客](https://blog.csdn.net/ybdesire/article/details/84946128)

**L1正则化：**

损失函数增加所有权重绝对值之和(**$|w_1|+|w_2|+\dots+|w_n|$**)，也即L1范数



L1正则化对所有参数的惩罚力度一样（求导均为1），可以让一部分权重变为0，产生稀疏模型，能够使某些特征的权重降为0

**L2正则化（岭回归）：**

损失函数增加所有权重平方和的平方根(**$\sqrt{w_1^2+w_2^2+\dots+w_n^2}$**)，也即L2范数

L2正则化减少了权重的固定比例（求导为$w_i$的正比），使权重平滑。L2正则化不会使权重变为0（不会产生稀疏模型），所以选择了更多的特征。

#### 区别

- L1减少的是一个常量，L2减少的是权重的固定比例
- L1使权重稀疏，L2使权重平滑
- L1优点是能够获得sparse模型，对于large-scale的问题来说这一点很重要，因为可以减少存储空间
- L2优点是实现简单，能够起到正则化的作用。缺点就是L1的优点：无法获得sparse模型



### 交叉熵损失函数和极大似然估计推导（2024.9.4）

参考：chatglm回答

**交叉熵损失函数：**

对样本$(x^{(i)},y^{(i)})$和其预测$\hat{y^{(i)}}$（这里$y$为独热编码，$\hat{y}$是一和$y$形状相同的向量，每个位置表示预测为该类的概率）

==$ Loss = -\Sigma_j y^{(i)}_j·log(\hat{y^{(i)}_j}) = -log(\hat{y^{(i)}_t})$==

其中${y^{(i)}_t}=1$，也即第$i$个样本对应的类别下标为$t$

#### 极大似然估计推导

对样本$(x^{(i)},y^{(i)})$和其预测$\hat{y^{(i)}}$**（这里认为$y^{(i)}$是对应类别的下标而非独热编码）**

要最大化的似然为$\prod_{i=1}^n\hat{y^{(i)}_{y^{(i)}}}$，对其取对数得$\sum_{i=1}^{n}ln\hat{y^{(i)}_{y^{(i)}}}$

故需要最小化的是$-\sum_{i=1}^{n}ln\hat{y^{(i)}_{y^{(i)}}}$，也即交叉熵损失函数



### 如何判断模型是否过拟合？(2024.9.5)

训练集loss低，测试集loss高不一定是过拟合

**参考：**

[回归模型偏差&方差&残差 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/50214504)

**模型偏差和方差：**

模型偏差(bias)：预测结果与真实值之间的差异，用于衡量模型的准确率

模型方差(variance)：多个模型之间比较的指标

![](./assets/bias_and_variance.jpg)



### 求似然函数步骤(2024.9.6)

- 定义：概率是给定参数，求某个事件发生概率；似然则是给定已发生的事件，估计参数。

1. 写出似然函数
2. 对似然函数取对数并整理
3. 求导数，导数为0处为最佳参数
4. 解似然方程



### softmax防止指数上溢工程化实现(2024.9.8)

参考：[万字秋招算法岗深度学习八股文大全 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/667048896)

# $softmax(x) = \frac{e^{x_i}}{\sum_ie^{x_i}} = \frac{e^{x_i-a}}{\sum_ie^{x_i-a}}$

（使$a$为$x_i$中的最大值）



### 常用优化器

参考：[万字秋招算法岗深度学习八股文大全 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/667048896)

#### SGD (2024.9.9)



